---
title: "ECo 602 - Analysis of Environmental Data"
subtitle: "Likelihood Basics"
author: "Michael France Nelson"
date: "Fall 2020"
output:
  beamer_presentation:
    pandoc_args: !expr paste0(here::here("formatting", "beamer", "eco_602_2020_beamer.yaml"))
    highlight: tango
    # theme: "default"
    colortheme: "spruce"
    fonttheme: "serif"
    slide_level: 2
    incremental: false
classoption: t
header-includes:
  \input{`r here::here("formatting", "beamer", "eco_602_2020_headers_tikz.tex")`}
  \usepackage{animate}
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(rmd.utils)
require(mfn.teaching.utils)
require(ggplot2)

y1 = 0.5

x = seq(-10, 10, length.out = 1000)

# lplot(c(1, -.3, 0), x, -0.0, .5)

lplot = function(
  y, x, 
  pop_mean, pop_sd, 
  l_col = 2, l_lwd = 1.4, 
  p_pch = 16, p_col = 1,
  main_fmt = "LL = %.5g",
  # main_fmt = "sum(log(L(x))) = %.5g",
  xlab = "", ylab = "")
{
  plot(
    x, 
    dnorm(x, mean = pop_mean, sd = pop_sd),
    type = "l", xlab = xlab, ylab = ylab
  )
  
  
  lik = 0
  
  for (y_i in y)
  {
    dens_i = dnorm(y_i, mean = pop_mean, sd = pop_sd)
    segments(
      x0 = y_i, x1 = y_i, 
      y0 = 0, y1 = dens_i,
      col = l_col, lwd = l_lwd)
    points(x = y_i, y = 0, pch = p_pch, col = p_col)
    
    lik = lik + log(dens_i)
  }
  title(main = sprintf(main_fmt, lik))
}


```


## What is Likelihood?

- In a statistical inference context, likelihood is related to the probability of observing a specific event, $x_i$, given a probability distribution and a set of parameters.
- We can use probability distribution functions to calculate the likelihood of specific events.
- The likelihood of an event is proportional to probability mass or density.



## Likelihood of Data:

- If we have more than one observation, the joint likelihood is the product of the probabilities of the individual events!
- *If* the data were independently collected/observed.



```{r, echo = FALSE, fig.height=5.5}

{
  y = rnorm(10, 0.1, 4.7)
  pm = 0.5; psd = 4.2
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
}
```







## Likelihood: The scenario

Main question: How likely am I to have observed the data I collected under my proposed model? 

Likelihood can help if you have: 

1. Data 
1. A proposed a distribution or model of the data 
1. A set of candidate distribution/model parameters 

For inference: it might seem reasonable to try to find the parameter values that make our observed data most likely.




## Maximum Likelihood

- Maximum likelihood method attempts to find the population parameters that make the
observed data the most likely.

```{r ml_demo_1, echo = FALSE, fig.height=4}
{
  y = c(-1, 0.2, 0.9, 2)
  
  pm = 0; psd = 1
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  {
    
    par(mfrow = c(1, 4))
    pm = 0; psd = 1
    mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
    lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
    
    pm = 1; psd = 1
    mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
    lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
    
    pm = 1; psd = 1.5
    mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
    lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
    
    pm = 2; psd = 1.5
    mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
    lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
  }
  
  
  
}


```







## Likelihood: independent samples 

- Since you are a whiz at designing experiments, you know that all of your samples are independent!
- What do we already know about the joint probability of multiple, independent events?




## Likelihood: independent samples

- The joint probability of observing multiple independent events is the product of the probabilities of the individual events.

- Likelihood is an estimate of how probable your particular data are given a model and a set of model parameters.

The overall likelihood is proportional to the product of the likelihoods of each observation given your model/parameters!




## Likelihood: data and model

How do we calculate the likelihood for a specific event or observation if we have a theoretical distribution?

- Use a density/mass function.


How do we calculate the likelihood for an entire sample if we have a theoretical distribution?

- Multiply the density/mass of each observation.



## Likelihood: procedure 

1. Collect data 
2. Propose model and candidate parameter values 
3. Calculate the probability density of each observation given your model and parameter values:
- From a theoretical distribution.
- From an empirical/resampled/simulated distribution




## Likelihood: procedure 

4. Multiply the densities.

- In practice we calculate the logarithm of the densities and add them together.
- Why might this be better than multiplying probabilities?

5. Voil√†: your likelihood value for your data $Y$ given your proposed model and parameter values: $\Phi_m$

- In symbols $L \left( Y \vert \Phi_m\right)$ 




## Maximizing the Likelihood:


\begin{blueborder}{0.9\textwidth}{[Log] Likelihoods: Why do we use logarithms?}
\begin{itemize}
\item{Floating Point Underflow: Products of decimals are \textit{extremely} small numbers.}
\item{Floating point double precision numbers cannot handle arbitrarily small quantities.}
\item{Logarithms transform products into addition/subtraction operations.}
\end{itemize}
\end{blueborder}




## Likelihood calculations

In Maximum Likelihood inference we want to maximize the [log] likelihood of the parameters.

Wouldn't it be nice if we had a simple formula? 

- Sometimes we can find a formula and then find it's minima/maxima via calculus.
- Frequently such formulas don't exist.



## Likelihood Example Calculations

- one point: $x = 0.5$
- Normal distribution: $\mu = -0.5$, $\sigma = 2.6$


```{r ex_calc_0, echo = FALSE, fig.height=5.5}

{
  
  y = 0.5
  pm = -0.5; psd = 2.6
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
}
```



## Likelihood Example Calculations

- one point: $x = 0.5$
- Normal distribution: $\mu = .85$, $\sigma = 2.6$


```{r ex_calc_1, echo = FALSE, fig.height=5.5}

{
  y = 0.5
  pm = 0.85; psd = 2.6
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
}
```





## Likelihood Example Calculations

- Multiple points
- Normal distribution: $\mu = 1.5$, $\sigma = 2.6$


```{r ex_calc_2, echo = FALSE, fig.height=5.5}
{
  y = c(-4.1, -1.9, 0.5, 4.1)
  pm = 1.5; psd = 2.6
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
}
```



## Likelihood Example Calculations

- Multiple points
- Normal distribution: $\mu = 0.5$, $\sigma = 2.6$


```{r ex_calc_3, echo = FALSE, fig.height=5.5}
{
  y = c(-4.1, -1.9, 0.5, 4.1)
  pm = 0.5; psd = 2.6
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
}
```




## Likelihood Example Calculations

- Multiple points
- Normal distribution: $\mu = 0.5$, $\sigma = 4.6$


```{r ex_calc_5, echo = FALSE, fig.height=5.5}
{
  y = c(-4.1, -1.9, 0.5, 4.1)
  pm = 0.5; psd = 4.6
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
}
```




## Likelihood Example Calculations

- Multiple points
- Normal distribution: $\mu = -3.5$, $\sigma = 3.2$


```{r ex_calc_5, echo = FALSE, fig.height=5.5}
{
  y = c(-4.1, -1.9, 0.5, 4.1)
  pm = -3.5; psd = 3.2
  mft = sprintf("mean = %2$.2f, sd = %3$.2f\nLL = %1$s", "%.5g", pm, psd)
  lplot(y, x, pop_mean = pm, pop_sd = psd, main_fmt = mft)
}
```





<!-- ## Likelihood visualization activity -->

<!-- Before we start, do you remember the Bernoulli distribution? -->

<!-- - What does it describe? -->
<!-- - What are its parameters? -->

<!-- Visit the Seeing Theory likelihood page Instructions and link are on Moodle  -->

