---
title: "ECo 602 - Analysis of Environmental Data"
subtitle: "Conditional Probabiltiy: Basics"
author: "Michael France Nelson"
date: "Fall 2020"
output:
    beamer_presentation:
    pandoc_args: !expr paste0(here::here("formatting", "beamer", "eco_602_2020_beamer.yaml"))
    highlight: tango
    # theme: "default"
    colortheme: "spruce"
    fonttheme: "serif"
    slide_level: 2
    incremental: false
classoption: t
header-includes:
  \input{`r here::here("formatting", "beamer", "eco_602_2020_headers_tikz.tex")`}
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(rmd.utils)
require(mfn.teaching.utils)
require(ggplot2)
```



## Relationship between model and data

Frequentist:

- Data are one realization of a stochastic sampling process
- The One True Model exists and is unknowable

Bayesian:

- We know that our data exist, they are not random
- The model is a random variable that we will estimate from our fixed data


## Uncertainty about the model

Frequentist:

- True model parameters are unknowable but fixed
- Model parameters have no distributions, they simply exist!

Bayesian:

- Model parameters are random, they have probability distributions that we estimate from our fixed data



## Confidence and Credibility

Frequentist 95% confidence interval:

- We are confident that our process would produce intervals containing the true value 95% of the time.
- Certainty about whether a particular interval contains the true value is tricky.

Bayesian 95% credible interval:

- Given our data, we are 95% certain that our particular interval contains the real parameter value




## Inference: Frequentist

1. Estimate parameters that make our data most likely, under the assumption that they
are one of infinite possible samples.
2. Express our parameter estimates in terms of a confidence intervals and p values.

- The CI either contains the param value or not. We can't know for a particular CI.





## Inference: Bayesian

1. Estimate probability distributions of the parameters that are most likely given our data, and previous data/knowledge.
- Conditional probability is key

2. Express our estimates in terms of credible intervals. P values aren't as important.




## Bayesian symbols and notation

Follows the format of conditional probability:

$Pr(A | B)$: What is the probability of A given that we know B occurred?

$Pr(H | D)$: What is the probability of our hypothesis (H) given that we have observed the data (D)




## Bayesian symbols:

Hypothesis comprises our proposed model and a set of model parameter values

- Often denoted $H$ or $\Phi_m$

Data comprises our current and previous data or knowledge

- Denoted $D$ or $Y$




## Four important probabilities/distributions

1. $Pr(Y)$: the probability or likelihood of our observed data
2. $Pr(\Phi_m)$: The probability distribution of our model and parameters before data are observed
- How could we possibly know this before we start?
- Prior probability from previous data, maybe?



## Four important probabilities/distributions

3. $Pr(Y|\Phi_m)$: Probability of observing the current data given our estimated model and the previous data.
- Likelihood function of the model parameters: we want to maximize this function

4. $Pr(\Phi_m|Y)$: Probability distribution of our estimated model parameters after the data are observed.
- This is what we want to infer!
- Posterior probability.



## Bayesian: what do we need to proceed?

1. $Pr(H)$: Prior unconditional distribution of the probability of our model params
2. $Pr(D)$: Unconditional probability of observing the current data:
  - This is difficult, but we don't have to know it directly.
3. $Pr(D|H)$: Conditional probability of observing our data given the model parameters.
  - Estimated is from the likelihood function.
  - Remember likelihood functions aren't trivial to find/define!




